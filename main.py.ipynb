{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a28da1f1",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241d005c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! pip install -U Chroma\n",
    "#! pip install -U langchain\n",
    "#! pip install requests\n",
    "#! pip install langchain-community\n",
    "#! pip install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ea470",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "LangChain simplifies every stage of the LLM application lifecycle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2dca386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "from langchain_community.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5619d9e0",
   "metadata": {},
   "source": [
    "## Initialize a LLM\n",
    "\n",
    "Mistral is an open source model (free unlike OpenAI)\n",
    "\n",
    "HuggingFace is an open source data science and machine learning platform. It acts as a hub for AI experts and enthusiastsâ€”like a GitHub for AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80cb452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/sampostelnik/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# huggingfacehub_api_token=\"your_api_token\"\n",
    "huggingfacehub_api_token=\"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\"\n",
    "\n",
    "conv_model = HuggingFaceEndpoint(\n",
    "    huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "    repo_id=model_id,\n",
    "    max_new_tokens=1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97fd38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ai_response(user_query, docs):\n",
    "    context = \"\"\n",
    "    # Turn the docs array into a string\n",
    "    for doc in docs:\n",
    "        context += f\"\\n\\n{doc.page_content}\"\n",
    "        \n",
    "    # print(context)\n",
    "    \n",
    "    # Prompt\n",
    "    template = \"\"\"[INST]\n",
    "    You are a financial expert in analyzing financial statements.\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "    Respond to the user input from the context below:\n",
    "    {context}\n",
    "    Question: {query} [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"query\"],\n",
    "    )\n",
    "\n",
    "    # print(f\"Prompt: {prompt}\")\n",
    "\n",
    "    # LLM Chain takes user input, formats it with a PromptTemplate, \n",
    "    # and then passes the formatted response to a ChatModel\n",
    "    llm_chain = LLMChain(llm=conv_model, prompt=prompt)\n",
    "\n",
    "    response = llm_chain.invoke({\"context\": {context}, \"query\": user_query})\n",
    "\n",
    "    response_formatted = response['text']\n",
    "\n",
    "    return response_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a30c6f",
   "metadata": {},
   "source": [
    "## Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378c5739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F I N A N C I A L   S U M M  A R Y\n",
      "(Unaudited)\n",
      "($ in millions, except percentages and per share data) 2019 2020 2021 2022 2023 YoY\n",
      "Total automotive revenues  20,821  27,236  47,232  71,462  82,419 15%\n",
      "Energy generation and storage revenue  1,531  1,994  2,789  3,909  6,035 54%\n",
      "Services and other revenue 2,226 2,306 3,802 6,091 8,319 37%\n",
      "Total revenues  24,578 31,536  53,823  81,462 96,773 19%\n",
      "Total gross profit  4,069 6,630  13,606  20,853 17,660 -15%\n",
      "Total GAAP gross margin 16.6% 21.0% 25.3% 25\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"TSLA-Q4-2023-Update.pdf\")\n",
    "# loader = PyPDFLoader(\"2023ar.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "page = pages[4]\n",
    "\n",
    "print(page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c62937e",
   "metadata": {},
   "source": [
    "## Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425c6813",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='F I N A N C I A L   S U M M  A R Y\\n(Unaudited)\\n($ in millions, except percentages and per share data) Q4-2022 Q1-2023 Q2-2023 Q3-2023 Q4-2023 YoY\\nTotal automotive revenues 21,307 19,963 21,268 19,625 21,563 1%\\nEnergy generation and storage revenue 1,310 1,529 1,509 1,559 1,438 10%\\nServices and other revenue 1,701 1,837 2,150 2,166 2,166 27%\\nTotal revenues 24,318 23,329 24,927 23,350 25,167 3%\\nTotal gross profit 5,777 4,511 4,533 4,178 4,438 -23%\\nTotal GAAP gross margin 23.8% 19.3% 18.2% 17.9% 17.6% -612 bp\\nOperating expenses 1,876 1,847 2,134 2,414 2,374 27%\\nIncome from operations 3,901 2,664 2,399 1,764 2,064 -47%\\nOperating margin 16.0% 11.4% 9.6% 7.6% 8.2% -784 bp\\nAdjusted EBITDA 5,404 4,267 4,653 3,758 3,953 -27%\\nAdjusted EBITDA margin 22.2% 18.3% 18.7% 16.1% 15.7% -652 bp\\nNet income attributable to common stockholders (GAAP) 3,687 2,513 2,703 1,853 7,928 115%\\nNet income attributable to common stockholders (non- GAAP) 4,106 2,931 3,148 2,318 2,485 -39%\\nEPS attributable to common stockholders, diluted (GAAP) 1.07 0.73 0.78 0.53 2.27 112%\\nEPS attributable to common stockholders, diluted (non- GAAP) 1.19 0.85 0.91 0.66 0.71 -40%\\nNet cash provided by operating activities 3,278 2,513 3,065 3,308 4,370 33%\\nCapital expenditures (1,858) (2,072) (2,060) (2,460) (2,306) 24%\\nFree cash flow 1,420 441 1,005 848 2,064 45%\\nCash, cash equivalents and investments 22,185 22,402 23,075 26,077 29,094 31%\\n4', metadata={'source': 'TSLA-Q4-2023-Update.pdf', 'page': 3})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(pages)\n",
    "\n",
    "documents[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418901a1",
   "metadata": {},
   "source": [
    "## Vectorstores and Embeddings\n",
    "\n",
    "Embedding is a meaningful representation of text in form of numbers.\n",
    "\n",
    "The embedding model is responsible for creating embeddings for the document chunks & user queries.\n",
    "\n",
    "Embeddings created by embedding model are stored in a vector store that offers fast retrieval and similarity search by creating an index over our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07bacccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceHubEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "persist_directory = 'docs/chroma'\n",
    "\n",
    "!rm -rf ./docs/chroma\n",
    "\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=huggingfacehub_api_token, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n",
    "\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=pages,\n",
    "    persist_directory=persist_directory,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d11f7",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba359a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question about the financial docs you uploaded: What is Tesla's Profitability?\n",
      "\n",
      "FinanceGPT: \n",
      "Based on the context provided, Tesla had GAAP operating income of $8.9 billion and GAAP net income of $15.0 billion in 2023. Additionally, there was a one-time non-cash tax benefit of $5.9 billion recorded in Q4. So, Tesla had a significant profitability in 2023. However, it's important to note that the context provides unaudited financial data.\n",
      "Do you want to ask another question?(y/n) y\n",
      "Ask a question about the financial docs you uploaded: What AI advancements has Tesla made?\n",
      "\n",
      "FinanceGPT: \n",
      " Tesla has made significant advancements in AI technology, specifically with the release of their latest FSD Beta software (V12) and the introduction of the 2nd generation Optimus robot. Both FSD Beta and Optimus utilize similar technology pillars: real-world data, neural net training, and cutting-edge hardware and software. The FSD Beta software utilizes end-to-end training, enhancing the driving experience, while the Optimus robot features improved AI capabilities and Tesla-designed actuators and sensors. Additionally, Tesla vehicles now offer high-fidelity 3D renderings of surroundings when parking, and wireless Bluetooth headphones can be paired with the rear screen. Tesla's focus on cost reductions across all points of production also contributes to their advancements in AI technology.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\n",
    "\n",
    "while(user_input != \"n\"):\n",
    "    user_input = input(\"Ask a question about the financial docs you uploaded: \")\n",
    "\n",
    "    # Similarity Search - get the documents that finds relevant documents to be passed into our llm\n",
    "    docs = vectordb.similarity_search(\n",
    "        user_input,\n",
    "        k=5,\n",
    "    )\n",
    "    \n",
    "#     for doc in docs:\n",
    "#         print(doc)\n",
    "\n",
    "    # Questions about Tesla\n",
    "    # What is Tesla's Profitability?\n",
    "    # What AI advancements has Tesla made?\n",
    "    \n",
    "    # Questions about Berkshire Hathaway\n",
    "    # What general businesss risks factors are discussed?\n",
    "    # How did Berkshire Hathaway perform in 2023?\n",
    "\n",
    "    response = get_ai_response(user_input, docs)\n",
    "\n",
    "    print(\"\\nFinanceGPT: \")\n",
    "    print(response)\n",
    "    user_input = input(\"Do you want to ask another question?(y/n) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60cce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
